import os
import json
import requests
import feedparser
import time
import re
import random
import warnings 
from datetime import datetime
from slugify import slugify
from io import BytesIO
from PIL import Image, ImageEnhance, ImageOps
from groq import Groq, APIError, RateLimitError, BadRequestError

# --- SUPPRESS WARNINGS ---
warnings.filterwarnings("ignore", category=FutureWarning, module="google.api_core")

# --- GOOGLE INDEXING LIBS ---
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build

# --- CONFIGURATION ---
GROQ_KEYS_RAW = os.environ.get("GROQ_API_KEY", "")
GROQ_API_KEYS = [k.strip() for k in GROQ_KEYS_RAW.split(",") if k.strip()]

# ðŸŸ¢ CONFIGURASI DOMAIN & INDEXNOW
WEBSITE_URL = "https://football-daily-two.vercel.app" 
INDEXNOW_KEY = "b0c1cebbd6004e1a9e25605cc51b2937" 
GOOGLE_JSON_KEY = os.environ.get("GOOGLE_INDEXING_KEY", "") 

if not GROQ_API_KEYS:
    print("âŒ FATAL ERROR: Groq API Key is missing!")
    exit(1)

# --- TIM PENULIS (NEWSROOM) ---
AUTHOR_PROFILES = [
    "Dave Harsya (Senior Analyst)", "Sarah Jenkins (Chief Editor)",
    "Luca Romano (Transfer Specialist)", "Marcus Reynolds (Premier League Correspondent)",
    "Elena Petrova (Tactical Expert)", "Ben Foster (footballs Journalist)",
    "Mateo Rodriguez (European Football Analyst)"
]

# --- ðŸŸ¢ DAFTAR KATEGORI RESMI WEBSITE ANDA ---
VALID_CATEGORIES = [
    "Transfer News", 
    "Premier League", 
    "Champions League", 
    "La Liga", 
    "International", 
    "Tactical Analysis"
]

# --- ðŸŸ¢ SUMBER RSS (US & UK) ---
RSS_SOURCES = {
    "US Source": "https://news.google.com/rss/search?q=Sports+News&hl=en-US&gl=US&ceid=US:en",
    "UK Source": "https://news.google.com/rss/search?q=Sports+News&hl=en-GB&gl=GB&ceid=GB:en"
}

# --- AUTHORITY SOURCES ---
AUTHORITY_SOURCES = [
    "Transfermarkt", "Sky footballs", "The Athletic", "Opta Analyst",
    "WhoScored", "BBC football", "The Guardian", "UEFA Official", "ESPN FC"
]

# --- FALLBACK IMAGES ---
FALLBACK_IMAGES = [
    "https://images.unsplash.com/photo-1508098682722-e99c43a406b2?auto=format&fit=crop&w=1200&q=80",
    "https://images.unsplash.com/photo-1431324155629-1a6deb1dec8d?auto=format&fit=crop&w=1200&q=80"
]

CONTENT_DIR = "content/articles"
IMAGE_DIR = "static/images"
DATA_DIR = "automation/data"
MEMORY_FILE = f"{DATA_DIR}/link_memory.json"

TARGET_PER_SOURCE = 3 

# --- MEMORY SYSTEM ---
def load_link_memory():
    if not os.path.exists(MEMORY_FILE): return {}
    try:
        with open(MEMORY_FILE, 'r') as f: return json.load(f)
    except: return {}

def save_link_to_memory(title, slug):
    os.makedirs(DATA_DIR, exist_ok=True)
    memory = load_link_memory()
    memory[title] = f"/{slug}"
    if len(memory) > 50:
        memory = dict(list(memory.items())[-50:])
    with open(MEMORY_FILE, 'w') as f: json.dump(memory, f, indent=2)

def get_formatted_internal_links():
    memory = load_link_memory()
    items = list(memory.items())
    if not items: return ""
    if len(items) > 3: items = random.sample(items, 3)
    formatted_links = []
    for title, url in items:
        formatted_links.append(f"* [{title}]({url})")
    return "\n".join(formatted_links)

# --- RSS FETCHER ---
def fetch_rss_feed(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://news.google.com/'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200: return None
        return feedparser.parse(response.content)
    except: return None

# --- CLEANING ---
def clean_text(text):
    if not text: return ""
    cleaned = text.replace("**", "").replace("__", "").replace("##", "")
    cleaned = cleaned.replace('"', "'") 
    cleaned = cleaned.strip()
    return cleaned

# --- IMAGE ENGINE ---
def download_and_optimize_image(query, filename):
    if not filename.endswith(".webp"):
        filename = filename.rsplit(".", 1)[0] + ".webp"

    base_prompt = f"{query} footballs action photography, stadium atmosphere, 8k resolution, highly detailed, photorealistic, cinematic lighting, sharp focus"
    safe_prompt = base_prompt.replace(" ", "%20")[:250]
    
    print(f"      ðŸŽ¨ Generating HQ Image: {base_prompt[:40]}...")

    for attempt in range(3):
        seed = random.randint(1, 999999)
        image_url = f"https://image.pollinations.ai/prompt/{safe_prompt}?width=1280&height=720&nologo=true&model=flux-realism&seed={seed}&enhance=true"
        
        try:
            response = requests.get(image_url, timeout=120)
            if response.status_code == 200:
                if "image" not in response.headers.get("content-type", ""):
                    time.sleep(2); continue

                img = Image.open(BytesIO(response.content)).convert("RGB")
                img = img.resize((1200, 675), Image.Resampling.LANCZOS)
                
                enhancer_sharp = ImageEnhance.Sharpness(img)
                img = enhancer_sharp.enhance(1.3)
                enhancer_color = ImageEnhance.Color(img)
                img = enhancer_color.enhance(1.1)

                output_path = f"{IMAGE_DIR}/{filename}"
                img.save(output_path, "WEBP", quality=75, method=6, optimize=True)
                
                print(f"      ðŸ“¸ HQ Image Saved: {filename}")
                return f"/images/{filename}" 

        except Exception as e:
            time.sleep(5)
    
    print("      âŒ Image failed after 3 attempts. Using Fallback.")
    return random.choice(FALLBACK_IMAGES)

# --- INDEXING ENGINE ---
def submit_to_google(url):
    if not GOOGLE_JSON_KEY: return
    try:
        creds_dict = json.loads(GOOGLE_JSON_KEY)
        SCOPES = ["https://www.googleapis.com/auth/indexing"]
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, SCOPES)
        service = build("indexing", "v3", credentials=credentials)
        body = {"url": url, "type": "URL_UPDATED"}
        service.urlNotifications().publish(body=body).execute()
        print(f"      ðŸš€ Google Indexing Submitted")
    except Exception as e:
        if "FutureWarning" not in str(e): print(f"      âš ï¸ Google Indexing Error: {e}")

def submit_to_indexnow(url):
    try:
        endpoint = "https://api.indexnow.org/indexnow"
        host = WEBSITE_URL.replace("https://", "").replace("http://", "")
        data = {
            "host": host,
            "key": INDEXNOW_KEY,
            "keyLocation": f"https://{host}/{INDEXNOW_KEY}.txt",
            "urlList": [url]
        }
        requests.post(endpoint, json=data, headers={'Content-Type': 'application/json; charset=utf-8'})
        print(f"      ðŸš€ IndexNow Submitted")
    except: pass

# --- AI WRITER ENGINE (UPDATED: UNIQUE HEADERS) ---
def parse_ai_response(text, fallback_title, fallback_desc):
    try:
        parts = text.split("|||BODY_START|||")
        if len(parts) >= 2:
            json_part = re.sub(r'```json\s*|```', '', parts[0].strip())
            data = json.loads(json_part)
            data['title'] = clean_text(data.get('title', fallback_title))
            data['description'] = clean_text(data.get('description', fallback_desc))
            data['image_alt'] = clean_text(data.get('image_alt', data['title']))
            data['content'] = parts[1].strip()
            return data
    except Exception: pass
    
    clean_body = re.sub(r'\{.*\}', '', text, flags=re.DOTALL).replace("|||BODY_START|||", "").strip()
    return {
        "title": clean_text(fallback_title),
        "description": clean_text(fallback_desc),
        "image_alt": clean_text(fallback_title),
        "category": "International", 
        "main_keyword": "Football",
        "lsi_keywords": [],
        "content": clean_body
    }

def get_groq_article_seo(title, summary, link, internal_links_block, author_name):
    valid_cats_str = ", ".join(VALID_CATEGORIES)
    
    # --- ðŸŸ¢ PERBAIKAN PROMPT UNTUK HEADER UNIK ---
    system_prompt = f"""
    You are {author_name} for 'football Daily'.
    
    TASK: Write a 1000+ word viral article based on the news.
    
    # CRITICAL INSTRUCTION FOR HEADERS (H2/H3):
    - **NEVER** use generic headers like "Analysis", "Deep Dive", "Conclusion", "Summary", "Introduction".
    - **ALWAYS** write unique, catchy headers that include specific Player Names, Team Names, or Action Verbs.
    - Example BAD: "## Tactical Analysis"
    - Example GOOD: "## How Mbappe's Speed Dismantled the Defense"
    
    # CRITICAL INSTRUCTION FOR CATEGORY:
    You must classify this news into EXACTLY ONE of these categories: [{valid_cats_str}].
    - If it's about NFL, NBA, or non-football footballs, use "International".
    - If it's about player movement/contracts, use "Transfer News".
    
    OUTPUT FORMAT (JSON):
    {{
        "title": "Headline (No Markdown)",
        "description": "Meta description",
        "category": "CHOOSE_FROM_LIST_ABOVE",
        "main_keyword": "Entity Name",
        "lsi_keywords": ["keyword1"],
        "image_alt": "Descriptive text for image"
    }}
    |||BODY_START|||
    [Markdown Content]

    # STRUCTURE:
    1. The Scoop: Intro Hook/Executive Summary.
    2. [Unique H2: Context of the Story].
    {{{{< ad >}}}}
    3. [Unique H2: Key Stats/Data Table] (Markdown Table).
    4. **Read More** (Paste Block Below).
    5. [Unique H2: Quotes & Reactions].
    6. FAQ (Questions must be specific to the topic).

    # INTERNAL LINKS BLOCK:
    ### Read More
    {internal_links_block}
    """

    user_prompt = f"News: {title}\nSummary: {summary}\nLink: {link}\nWrite it now."

    for api_key in GROQ_API_KEYS:
        client = Groq(api_key=api_key)
        try:
            print(f"      ðŸ¤– AI Writing (Unique Headers & Categorizing)...")
            completion = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.8, # Sedikit dinaikkan agar lebih kreatif judulnya
                max_tokens=7500,
            )
            return completion.choices[0].message.content
        except RateLimitError: continue
        except Exception as e: print(f"      âš ï¸ Error: {e}"); continue
            
    return None

# --- MAIN LOOP ---
def main():
    os.makedirs(CONTENT_DIR, exist_ok=True)
    os.makedirs(IMAGE_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)

    total_generated = 0

    for source_name, rss_url in RSS_SOURCES.items():
        print(f"\nðŸ“¡ Fetching Source: {source_name}...")
        feed = fetch_rss_feed(rss_url)
        if not feed or not feed.entries: continue

        cat_success_count = 0
        for entry in feed.entries:
            if cat_success_count >= TARGET_PER_SOURCE: break

            clean_title = entry.title.split(" - ")[0]
            slug = slugify(clean_title, max_length=60, word_boundary=True)
            filename = f"{slug}.md"

            if os.path.exists(f"{CONTENT_DIR}/{filename}"): continue

            current_author = random.choice(AUTHOR_PROFILES)
            print(f"   ðŸ”¥ Processing: {clean_title[:40]}... (Author: {current_author})")
            
            links_block = get_formatted_internal_links()
            
            raw_response = get_groq_article_seo(clean_title, entry.summary, entry.link, links_block, current_author)
            
            if not raw_response: continue

            data = parse_ai_response(raw_response, clean_title, entry.summary)
            if not data: continue

            if data['category'] not in VALID_CATEGORIES:
                data['category'] = "International"

            img_name = f"{slug}.webp"
            keyword_for_image = data.get('main_keyword') or clean_title
            final_img = download_and_optimize_image(keyword_for_image, img_name)
            
            date = datetime.now().strftime("%Y-%m-%dT%H:%M:%S+00:00")
            tags_list = data.get('lsi_keywords', [])
            if data.get('main_keyword'): tags_list.append(data['main_keyword'])
            tags_str = json.dumps(tags_list)
            img_alt = data.get('image_alt', clean_title).replace('"', "'")
            
            md = f"""---
title: "{data['title']}"
date: {date}
author: "{current_author}"
categories: ["{data['category']}"]
tags: {tags_str}
featured_image: "{final_img}"
featured_image_alt: "{img_alt}"
description: "{data['description']}"
slug: "{slug}"
url: "/{slug}/"
draft: false
---

{data['content']}

---
*Source: Analysis by {current_author} based on international reports and [Original Story]({entry.link}).*
"""
            with open(f"{CONTENT_DIR}/{filename}", "w", encoding="utf-8") as f: f.write(md)
            if 'title' in data: save_link_to_memory(data['title'], slug)
            
            print(f"   âœ… Published: {filename} (Category: {data['category']})")
            
            full_url = f"{WEBSITE_URL}/{slug}/"
            submit_to_indexnow(full_url)
            submit_to_google(full_url)
            
            cat_success_count += 1
            total_generated += 1
            time.sleep(5)

    print(f"\nðŸŽ‰ DONE! Total: {total_generated}")

if __name__ == "__main__":
    main()
